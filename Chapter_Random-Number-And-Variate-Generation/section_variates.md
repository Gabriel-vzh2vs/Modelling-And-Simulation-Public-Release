
(sec:random_variates)=
# Generating random variates

In {ref}`sec:random_number_generation` we covered generating random
number from from the standard uniform distribution $U(0,1)$. In this
section we will see that once you are armed with this, you can
generate variates from a range of distributions. Here we will go
through four techniques for this:

- The Inverse Transform Method
- Composition
- Convolution
- Rejection sampling

[A brief section on why one should learn this]

[Continuous and discrete]

(sec:inverse_transform_method)=
# The Inverse Transform Method

The setting is the following: we are given a univariate statistical
distribution with cumulative distribution function $F(x)$. The goal is
to generate variates (or sample) from this statistical
distribution. In the basic version we require that $F(x)$ is strictly
increasing for $0 < F(x) < 1$. We will later see how this can be
relaxed.

__Algorithm:__

  1. Generate $u$ from $U(0,1)$
  2. Return $x=F^{-1}(u)$


How do we know this is correct? What do we have to demonstrate? We
need to demonstrate that the random variable $X$ given by $F^{-1}(U)$
has CDF given by $F$.

\begin{align*}
 \Pr(X \le x) &= \Pr\bigl(F^{-1}(U) \le x \bigr)\\
	  &= \Pr\bigl(U \le F(x)\bigr) \\
	  &= F(x)\;.
\end{align*}

__Question:__ where was the monotonicity of $F$ used?

Is there any intuition behind the method?


(sec:composition_method)=
# The Composition Method

The composition method is a technique used to generate random variates from a target distribution $F(x)$ that can be
expressed as a mixture (or "composition") of several simpler (and ideally convex) component distributions.

And our target distribution must be able to be written in the form:

$$F(x) = \sum_i p_i \cdot F_i(x)$$

Where each $F_i(x)$ is a Cumulative Distribution Function (CDF) and $p_i$ represents the weights that
ensure $\sum_i P_i = 1$.

__Algorithm:__

  1. Generate a positive, random integer $I$ such that $P(I = i) = p_i$
  2. Return $X$ with CDF $F_i$ (given $I = i$, $X$ is independent of $I$).

The following proof demonstrates that the variate $X$ generated by the algorithm above
does converge into $F(x)$ when (2) is met when using the law of total probability and the
definition of a CDF.

:::{prf:proof} Composition Proof
:class: dropdown
For any fixed $x$,

```{math}
:label: Composition

P(X \le x) = \sum_{i} P(X \le x | I = i) \cdot P(I = i) \\

= P(X < x | I = i) \cdot  p_i\\

= \sum_{i} F_i(x) \cdot p_i \\

= F(x) \\
```

:::

__Question:__ The proof of the composition method above relies on the Law of Total Probability, $F(x) = \sum_{i} F_i(x) p_i$. What property of the $p_i$ weights is necessary for $F(x)$ to be a valid Cumulative Distribution Function (CDF)?

:::{prf:example} Symmetric Triangular Distribution
Example 1: Symmetric Triangular Distribution with vertical symmetry on $[-2, 2]$.
:::

(sec:convolution_method)=
# The Convolution Method

Convolutions are used when a random variable can be expressed as the sum
of two or more random variables, $Y_i$, and then this sum needs to be sampled as a random variate, $X$.
This is similar to the composition method, but expresses
the random variable as a sum of other random variates instead of the CDF as a weighted sum of other CDFs.

__Algorithm:__

  1. Generate $Y_1, Y_2, ... Y_m$ independently using their distribution
  2. Return $X = Y_1 + Y_2 + ... Y_m$

How do we use convolution?

:::{prf:example} Bernoulli converging to Binominal

A common use of convolution is hidden in of the most common
distributions, the binominal distribution. Fundamentally, all
binominal random variates are just a sum of i.i.d
Bernoulli random variates.

__Definition__: Let $Y = \sum_{i=1}^n X_i$, where $X_i$ are $n$ i.i.d. Bernoulli($p$) random variables. Then $Y \sim \text{Binomial}(n, p)$.

__Proof__:
We want to find the probability $P(Y=k)$ for $k \in \{0, 1, ..., n\}$.$Y=k$ means that exactly $k$ of the $X_i$ variables are equal to 1 and $n-k$ are equal to 0.

Consider one specific sequence of $k$ 1s and $n-k$ 0s (e.g., $k$ successes first, then $n-k$ failures). Because all $X_i$ are independent, the probability of this single sequence is:

$$P(X_1=1, ..., X_k=1, X_{k+1}=0, ..., X_n=0) = p^k (1-p)^{n-k}$$

The total number of such arrangements is given by $\binom{n}{k}$. Since each of these $\binom{n}{k}$ sequences is disjoint (a different outcome) and has the same probability $p^k (1-p)^{n-k}$, the total probability $P(Y=k)$ is the sum of their probabilities. Thus
$$P(Y=k) = \binom{n}{k} p^k (1-p)^{n-k}$$

This is the exact Probability Mass Function (PMF) of a Binomial($n, p$) random variable. Thus, $Y$ is binomially distributed.

And if you don't believe this result, we can also show a comparable result
using the Crude Monte Carlo method (CMC) to simulate a series of Bernoulli
Random Variates into a Binominal Random Variate, $W \sim Binominal(6, 0.4)$. 

![image](../Figs/Figure_1.png)

:::{admonition} Simulation Code
:class: dropdown

```{code} python
import numpy as np
import pandas as pd
from scipy.stats import binom
from plotnine import (ggplot, aes, geom_histogram, geom_point, theme_minimal, labs)

np.random.seed(2025)
n_samples = 1_000_000
x = np.random.binomial(5, 0.4, n_samples)
y = np.random.binomial(1, 0.4, n_samples)
w = x + y

print(f"Mean of x: {x.mean():.4f}") # 2.003
print(f"Mean of y: {y.mean():.4f}") # 0.3997
print(f"Mean of w: {w.mean():.4f}") # x + y = 2.4

df_w = pd.DataFrame({'w': w})
k = np.arange(7)
pdf = binom.pmf(k, 6, 0.4)
df_pdf = pd.DataFrame({'k': k, 'pdf': pdf})
hist_breaks = np.arange(-0.5, 7.5, 1)

p = (ggplot(df_w, aes(x='w'))
    + geom_histogram(
        aes(y='..density..'),
        breaks=hist_breaks,
        fill="gray",
        color="black"
    )
    + geom_point(
        data=df_pdf,
        mapping=aes(x='k', y='pdf'),
        color="black",
        size=3
    )
    + labs(
        title="BINOM(6, 0.4)",
        y="Density",
        x="w"
    )
    + theme_minimal()
)
p.show()
```

:::

:::

:::

(sec:rejection_sampling)=
# Rejection Sampling

Acceptance-rejection sampling is usually used when there is not a tractable, closed-form
expression for the target distribution's CDF $F(x)$. The goal is to generate variates $X$ from the density function $f
(x)$ of the target distribution. A requirement is that we must select a function $t(x)$ that _majorizes_ $f(x)$ for all
of $x$. However, t(x) is not a density, therefore, we need to set $c$ which is defined as $\int^{\infty}_{-\infty} t(x) dx \ge 1$,
and then define $d$ as a density that applies for all of $x$ as $d(x) = \frac{t(x)}{c}$.

__Algorithm:__

  1. Generate $Y$ having density d
  2. Generate $U$ from $U(0,1)$, independent of $Y$
  3. If $U \le \frac{f(Y)}{t(Y)}$, return $X = Y$ and stop (accept), else return to step 1 (reject)

The proof below shows how the algorithm given enough samples will converge into $f(x)$ using the
definition of conditional probabilities, law of total probability, calculus, and basic algebra.

:::{prf:proof} Long Proof: Rejection Sampling
:class: dropdown
This proof is based on the Rejection Sampling method from {cite:t}`liu2001monte`.

We get a $X$ conditional on acceptance from step 3, therefore, by the definition of conditional probabilities:

$$P(X \le x) = \frac{P(\text{acceptance}, Y \le x)}{P(\text{acceptance})}$$

And for any $y$:
$$P(\text{acceptance}| Y = y) = P(U \le \frac{f(y)}{t(y)}) = \frac{f(y)}{t(y)}$$

Because in step 2, we defined $U \sim U(0,1)$, and $Y$ is independent of $U$, and t(y) majorizes f(y), therefore:

```{math}
P(\text{acceptance, } Y \le x)= \int^{\infty}_{-\infty} P(\text{acceptance, } Y \le x| Y = y) \cdot r(y) dy
```

When then spilt this into the sum of two integration regions, the acceptance range and the rejection range
(aka what is below X and what is above X, respectively).

```{math}
\int^{X}_{-\infty} P(\text{acceptance, } Y \le x | Y = y) \cdot r(y) dy + \int^{\infty}_{X}P(\text{acceptance}, Y \le x | Y = y) \cdot r(y) dy
```

Which then simplifies into

$$\int^{X}_{-\infty} P(\text{acceptance}, Y \le x | Y = y) \cdot r(y) dy$$

And once we substitute in our definition of r(y)

$$\frac{1}{c} \int^{x}_{-\infty} t(y) dy$$

Which simplifies to

$$\frac{F(x)}{C}$$

However, we need to show how to reobtain $F(x)$, our original function from this simplification.
In this case, we can obtain $\frac{1}{c}$ from our probability of acceptance after substituting our
$r(y)$ and performing simplification.

$$P(\text{acceptance}) = \int^{\infty}_{-\infty} P(\text{acceptance}| Y = y) \cdot r(y) dy $$

Which becomes

$$\frac{1}{c} \int^{\infty}_{-\infty} \frac{f(y)}{t(y)} t(y) dy$$

Then we apply the multiplication of reciprocals (which always become 1), and simplify $\int^{\infty}_{-\infty} F(x)$
as one because it is a density, and therefore also equal to one, to get $\frac{1}{c}$.

Finally, we substitute our $P(\text{acceptance, } Y \le x)$ and $P(\text{acceptance})$ into the our definition of the
conditional probability, $\frac{P(\text{acceptance, } Y \le x)}{P(\text{acceptance})}$, giving us $F(x)$ through
algebraic manipulation.

$$\frac{F(x)/c}{1/c} \rightarrow F(x)$$
:::

__Question__: The method requires an "envelope" $c \cdot g(x)$ such that $f(x) \le c \cdot g(x)$. What property of the constant $c$ is essential for the acceptance probability $P(\text{acceptance})$ to be valid, and where in the proof is this used?

Now, how can we use rejection sampling?

:::{prf:example} Half-Normal Random Variable


Example 1 from Ross, Generate a standard half-normal RV with PDF:

$$a$$
:::
[Example Here]


# Problem Set for the Reader

:::{seealso} Problem 1 (Software Implementation of Random Variate)
Supposed that a Random Variable, $B$ has the following p.d.f:

$$f(x) = \begin{cases}
    0 & \text{if } x < 0 \text{ or } x > 2 \\
    x & \text{if } 0 \le x \le 1 \\
    \frac{1}{2} & \text{if } 1 \le x \le 2
\end{cases}$$

1) Use a method for generating realizations $B$ through any of the four methods, some are likely to
be easier than others.

2) Write a Python function using numpy that implements the inverse transform method for this p.d.f.

3) How would you computationally verify that your simulation method is correct?

4) Compare the Inverse Transform method and the Rejection Sampling method for this specific p.d.f.
:::

:::{seealso} Problem 2 (Analytic Application: Machine Failure)
A machine is taken out of production either if it fails or after a period of 7 hours. By running
similar machines until failure, it has been found that time to failure, $F$, has the Weibull distribution with
$\alpha = 9, \beta = 0.55, \text{ and } \nu = 0$.

{style=lower-roman}
1) Write out a step-by-step procedure for generating the time, $X$, until the machine is out of production. _Hint_: This can be expressed through $X = min(F, 7)$.
2) Calculate the probability is the machine is taken out of production exactly at 7 hours, $P(X=7)$, then the expected time that machine is in production, $E[X]$.
3) Using Python and Crude Monte Carlo, Empirically Estimate $E[X]$ and $P(X=7)$ using $n = 1,000$ samples.
4) Compare your Empirical Estimate to your Analytical Answer from Problem 2, 2.

:::

:::{seealso} Problem 3 (Application)
Example 1: Symmetric Triangular Distribution with vertical symmetry on $[-2, 2]$.
:::

:::{seealso} Problem 4
Example 1: Symmetric Triangular Distribution with vertical symmetry on $[-2, 2]$.
:::

:::{seealso} Problem 5
Example 1: Symmetric Triangular Distribution with vertical symmetry on $[-2, 2]$.
:::
