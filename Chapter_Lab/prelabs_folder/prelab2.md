(prelab-5)=
# Pre-Lab 5: Tutorial for Basic Monte Carlo Method (Do)

This pre-lab uses XLrisk (for Excel users) and pyMC + pandas packages (for Python Users)
as the main implementation methods; however, these all of these tasks can be
done within other packages in python such as the monaco and (copulas or statsmodels) packages.

In this pre-lab we will discuss the Monte Carlo Method with its implementations
and do a short example of coin-flipping that loosely relates to Lab-2. Keep in mind that
the pyMC section depends on the XLRisk section to reduce duplication.

:::{tab-set}

## Correlations between Random Variables [^3]

In this pre-lab, we should relate a random variable to another random variable in a joint distribution.
One method for doing so is building a distribution that connects related (dependent) variables.
An example of these kind of dependent variables would be if a person goes to an expensive hotel,
it is more likely than not that they would get expensive food, tours, and everything else on their trip.

It is possible to visualize this relation between dependent variables, and example of this is below.

```{figure} #fig:copula
:label: fig:copula-1

An example of the correlation between Accommodation Prices and Meal Costs through a joint distribution
this is related to {ref}`project-1`.

```

:::{tab-item} XLRisk

### Functions

In XLRisk, there is a series of functions for defining a random variate, of which the bare
minimum for this pre-lab are here, with a more extensive list in {ref}`sec:software`.

- =RiskBernolli() # For a Bernoulli Variate
- =RiskUniform() # For a Uniform Variate
- =RiskCorMat() # For Copulas

In XLrisk, this copulas are implemented through the function RiskCorMat, which takes a matrix and applies it to
random variates generated by XLRisk based on user parameters.

### Trials and Outputs

A trial in XLrisk consists of the generation of random numbers through
_Wichmann–Hill_[^1] pseudorandom generator which are then transformed into
a random variate through a random variate generator (which will be discussed in detail later)
and through an output function it becomes simulated values. These values are then
sampled through a _latin hypercube_ process, then mathematical
operations (such as adding and subtracting output values) are performed
With the statistic components (calculating mean, variance and skewness) existing
This might be a bit confusing at first, but more information is available
in {ref}`sec:prob_stats`, and for the general process known as the Monte Carlo Method
it is described in {ref}`sec:monte_carlo_method`.

Often times, in XLrisk, a user will pick a number of iterations, and through the Law of Large Numbers[^2],
the more trials, the more likely the sample average from those trials will converge into the true average value.

### Skew and Kurtosis

Skewness refers the asymmetry of the probability distribution of a
real-valued random variable about its mean. Skewness has two types.
*Positive* Skewness means that the distribution has a longer or fatter tail
on the right side and the distribution's PMF is concentrated on the left. The mean is typically greater than the median.
*Negative* Skewness mean that The distribution has a longer or fatter tail on the
left side and the mass of the distribution is concentrated on the right. The mean is typically less than the median.

Kurtosis refers to the "tailedness" (or "tail-heaviness")
of the probability distribution of a real-valued random variable.
It describes the sharpness of the peak and the weight of the tails
relative to a normal distribution. Kurtosis generally has three
forms. *Mesokurtic* ($\gamma \approx 0$) The distribution has a kurtosis
similar to that of a normal distribution. Tails are neither particularly
heavy nor light. *Leptokurtic* ($\gamma_2 > 0$) The distribution has heavier
tails and a sharper peak than a normal distribution. This means there is a
higher probability of extreme values (outliers). *Platykurtic*
($\gamma < 0$) The distribution has lighter tails and a flatter peak
than a normal distribution. This means there's a lower probability of extreme values,
and values tend to be more clustered around the mean but less
peaked than a normal distribution.

Skewness and Kurtosis are important for understanding the behavior of the results of the
model, particularly when calculating the likelihood of extreme events, which is defined as risk in many different settings.

### Walk-Through (Virtual Textbook Only)

Here is a video describing probabilistic risk analysis using XLrisk that is similar to
one of the projects that is within this text that uses Python instead of XLrisk to ensure
that the reader will understand how Monte Carlo works.

```{iframe} https://www.youtube.com/watch?v=0RiEquwDjNg
This video is a about 20 minutes long and describes a project for 
building a risk estimate leveraging XLrisk. This is a part of the XLrisk
documentation.
```

:::

:::{tab-item} pyMC and its Ecosystem

### pyMC Functions

See {ref}`sec:software` for specifics on what might be useful for
pyMC, as it uses Numpy and Scipy functions to perform actions, and
some PyMC functions are included toward the end of the document.

### pyMC Correlations between Variables

In pyMC, correlation between variables are implemented through a five step process based on
{cite}`PyMCDocumentation`:

1. Define the marginal distributions (they can be any distribution that is univariate);
2. Transform the marginals into the inverse of the Gaussian CDF (this is known as the Probability integral transform).
3. Define the multivariate Gaussian instantiated with the desired correlation matrix between the marginals;
4. Then simulate (or obtain the actual values from) a multidimensional Gaussian.
5. Apply the probability integral transformation to the multidimensional Gaussian.
6. Replace the marginals of the multidimensional Gaussian into the desired marginal distributions from step 2.

An example of this transformation between two exponential random variables with parameters three and five is included below:

::::{admonition} Code
:class: dropdown

```{code} python
import matplotlib.pyplot as plt
import numpy as np
import pymc as pm
import seaborn as sns

from scipy.stats import expon, multivariate_normal, norm

# Step (1,2, and 3), define the multivariate Gaussian:
b_scale = 2
θ = {"a_dist": norm(), "b_dist": expon(scale=1 / b_scale), "rho": 0.9}

n_samples = 5000

# Step 4: sample the multivariate Gaussian:
mu = [0, 0]
cov = [[1, θ["rho"]], [θ["rho"], 1]]
x = multivariate_normal(mu, cov).rvs(n_samples, random_state=rng)
a_norm = x[:, 0]
b_norm = x[:, 1]

# Step 5: Apply the Transformation to the multivariate Gaussian:
a_unif = norm(loc=0, scale=1).cdf(a_norm)
b_unif = norm(loc=0, scale=1).cdf(b_norm)
sns.jointplot(x=a_unif, y=b_unif, height=6, kind="hex");

# Step 6.
a1 = ["a_dist"behavior].cdf(a)
b1 = ["b_dist"].cdf(b)
sns.jointplot(x=a1, y=b1, kind="hex", height=6);
```
::::

### pyMC Presentation of Results

The differences between PyMC and XLrisk are not based on fundamental mathematical
differences, but based on interface changes between a spreadsheet and Python,
as the latter tends to have non-graphical UI/UX. And the user is expected to define the
interface through its commands and some of it is listed in {ref}`sec:software`.
An case study is included below to assist the reader.

### Monte Hall Case Study - pyMC

There is a classical problem in Probability called the Monte Hall Problem, and it goes something like this (Savant, 1990):

Suppose you're on a game show, and you're given the choice of three doors:
Behind one door is a car; behind the others, goats.
You pick a door, say No. 1, and the host, who knows what's behind the doors, opens another door,
say No. 3, which has a goat. He then says to you,
"Do you want to pick door No. 2?" Is it to your advantage to switch your choice?

The answer might seem obvious or impossible depending on your mathematical background. However, we can solve this using
the power of simulation, while also determining the skewness and kurtosis of potential outcomes.

In this case, the solution to this problem is based on Marilyn vos Savant's article in Parade from 1990 with some modern
influences from [Austin's Introduction to PyMC](https://austinrochford.com/posts/intro-prob-prog-pymc.html#The-Monty-Hall-problem).

And through a Monte Carlo Simulation, we get the following as the solution to the Monte Hall Problem.

```{code}
--- Monty Hall Problem Simulation Results ---
Number of simulated games: 10000
Probability of winning if you STAY: 33.55%
Probability of winning if you SWITCH: 66.45%

--- Distribution Shape Statistics ---
STAY Strategy Skewness: 0.6968
STAY Strategy Kurtosis: -1.5145
SWITCH Strategy Skewness: -0.6968
SWITCH Strategy Kurtosis: -1.5145

--- Preview of 5 games ---
Game 1: Prize is behind door 1. Contestant chose 2. Monty opened 0. Switching goes to 1. Result (Stay/Switch): Lose/Win
Game 2: Prize is behind door 0. Contestant chose 2. Monty opened 1. Switching goes to 0. Result (Stay/Switch): Lose/Win
Game 3: Prize is behind door 1. Contestant chose 2. Monty opened 0. Switching goes to 1. Result (Stay/Switch): Lose/Win
Game 4: Prize is behind door 0. Contestant chose 0. Monty opened 1. Switching goes to 2. Result (Stay/Switch): Win/Lose
Game 5: Prize is behind door 2. Contestant chose 2. Monty opened 0. Switching goes to 1. Result (Stay/Switch): Win/Lose

```

::::{admonition} Code for the Monte Hall Problem in Python
:class: dropdown

```{code} python
import pymc as pm
import pytensor.tensor as pt
import numpy as np
import arviz as az
import scipy.stats as stats

# The doors are numbered 0, 1, and 2.
DOORS = [0, 1, 2]
NUM_SAMPLES = 10000

with pm.Model() as monty_hall_model:

    # These are the initial beliefs about the random variables before we see any data.
    # The prize is behind one of three doors with equal probability (1/3).
    prize_door = pm.Categorical('prize_door', p=[1/3, 1/3, 1/3])
    initial_choice = pm.Categorical('initial_choice', p=[1/3, 1/3, 1/3])

    def monty_opens_func(prize, choice):
        all_doors = pt.arange(3)
        # Doors Monty can open if the contestant's choice is correct
        remaining_doors_when_correct = all_doors[~pt.eq(all_doors, choice)]
        # In the PyMC model, we must be deterministic. If the contestant is right,
        # we'll assume Monty always opens the lower-numbered of the two available doors.
        # This choice doesn't affect the final probabilities.
        monty_if_correct = remaining_doors_when_correct[0]
        # Door Monty must open if the contestant's choice is incorrect
        monty_if_incorrect = 3 - prize - choice

        # Use pt.switch to decide which case applies.
        return pt.switch(pt.eq(prize, choice), monty_if_correct, monty_if_incorrect)

    # pm.Deterministic creates a variable whose value is strictly determined by its parents.
    monty_opens = pm.Deterministic('monty_opens', monty_opens_func(prize_door, initial_choice))
    
    def switch_choice_func(choice, monty):
        return 3 - choice - monty

    switch_choice = pm.Deterministic('switch_choice', switch_choice_func(initial_choice, monty_opens))
    win_if_stay = pm.Deterministic('win_if_stay', pt.eq(initial_choice, prize_door))
    win_if_switch = pm.Deterministic('win_if_switch', pt.eq(switch_choice, prize_door))

# pm.sample_prior_predictive does exactly this: it draws samples from the prior distributions
# and calculates the values of the deterministic variables.
with monty_hall_model:
    prior_trace = pm.sample_prior_predictive(samples=NUM_SAMPLES, random_seed=42)

# The trace contains the outcomes of all 10,000 simulated games.
prob_win_stay = prior_trace.prior['win_if_stay'].mean().item()
prob_win_switch = prior_trace.prior['win_if_switch'].mean().item()

# Extract the boolean outcomes (True/False) and convert them to numbers (1/0)
stay_outcomes = prior_trace.prior['win_if_stay'].values.flatten().astype(float)
switch_outcomes = prior_trace.prior['win_if_switch'].values.flatten().astype(float)

# Calculate skewness for each strategy
skew_stay = stats.skew(stay_outcomes)
skew_switch = stats.skew(switch_outcomes)

# Calculate kurtosis for each strategy
kurt_stay = stats.kurtosis(stay_outcomes)
kurt_switch = stats.kurtosis(switch_outcomes)

print("\n--- Monty Hall Problem Simulation Results ---")
print(f"Number of simulated games: {NUM_SAMPLES}")
print(f"Probability of winning if you STAY: {prob_win_stay:.2%}")
print(f"Probability of winning if you SWITCH: {prob_win_switch:.2%}")

# --- ADDED: Print Skewness and Kurtosis Results ---
print("\n--- Distribution Shape Statistics ---")
print(f"STAY Strategy Skewness: {skew_stay:.4f}")
print(f"STAY Strategy Kurtosis: {kurt_stay:.4f}")
print(f"SWITCH Strategy Skewness: {skew_switch:.4f}")
print(f"SWITCH Strategy Kurtosis: {kurt_switch:.4f}")
# --- END ADDED SECTION ---

print("\n--- Preview of 5 games ---")
for i in range(5):
    prize = prior_trace.prior['prize_door'].values[0, i]
    choice = prior_trace.prior['initial_choice'].values[0, i]
    monty = prior_trace.prior['monty_opens'].values[0, i]
    sw_choice = prior_trace.prior['switch_choice'].values[0, i]
    stay_win = "Win" if prior_trace.prior['win_if_stay'].values[0, i] else "Lose"
    switch_win = "Win" if prior_trace.prior['win_if_switch'].values[0, i] else "Lose"
    print(
        f"Game {i+1}: Prize is behind door {prize}. "
        f"Contestant chose {choice}. Monty opened {monty}. "
        f"Switching goes to {sw_choice}. "
        f"Result (Stay/Switch): {stay_win}/{switch_win}"
    )
```

::::

:::

::::

[^1]: It is important to note that the Wichmann-Hill random number generator is based on three Linear Congruential Generators (LCGs) that have different prime modulus (2^31-1, 8121, and 48271) for example that are then combined into one stream, this process was discussed in {cite}`Law:13`.

[^2]: In several simulation textbooks such as {cite}`Law:13` and {cite}`Banks:14`, they describe that this property is universal and inevitable through the _strong law of large numbers_, when that is not always true, particularly with some of the projects and labs in this text. An example of a limitation of the law of large number is that some CDFs do not have an expected value, but the weak law still holds in these cases.

[^3]: This is generally referred to as a copula.
