(prelab-2)=
# Pre-Lab 2: Tutorial for Pre-Packaged Monte Carlo Methods (Do)

This pre-lab uses XLrisk (for Excel users) and pyMC + pandas packages (for Python Users)
as the main implementation methods; however, these all of these tasks can be
done within other packages in python such as the monaco and (copulas or statsmodels) packages.

In this pre-lab we will discuss the Monte Carlo Method with its implementations
and do a short example of coin-flipping that loosely relates to Lab-2. Keep in mind that
the XLRisk and pyMC sections are almost identical on purpose.

::::{tab-set}

### Correlations between distributions (Copula)

In this pre-lab, we consider copulas as forcing a distribution to assume the
behavior of another one, this is known as correlation. An example of this would be if a person goes to an
expensive hotel, it is more likely that they would get expensive food, tours, and everything else on their trip.

In more formal terms, based on Sklar's Theorem, a copula is a multivariate distribution such that marginalizing gives
a uniform $[0,1]$, and the marginal distributions are inherently uncorrelated with other, and the correlation is
exclusively provided by the copula; here is an example of a Gaussian copula from a completed {ref}`project-1`. Copulas are dicussed in more detail in a future chapter.

```{figure} #fig:copula
:label: fig:copula-1

An example of the correlation between Accommodation Prices and Meal Costs through a Correlation Matrix,
this is related to {ref}`project-1`.

```

:::{tab-item} XLRisk

### Functions

In XLRisk, there is a series of functions for defining a random variate, of which the bare
minimum for this pre-lab are here, with a more extensive list in {ref}`sec:software`.

- =RiskBernolli() # For a Bernoulli Variate
- =RiskUniform() # For a Uniform Variate
- =RiskCorMat() # For Copulas

In XLrisk, this copulas are implemented through the function RiskCorMat, which takes a matrix and applies it to
random variates generated by XLRisk based on user parameters.

### Trials and Outputs

A trial in XLrisk consists of the generation of random numbers through
_Wichmann–Hill_[^1] pseudorandom generator which are then transformed into a random variate through a random variate generator (which is defined by a distribution such as a uniform or a Gaussian distribution) and through an output function it becomes simulated values. These values are then sampled through a _latin hypercube_ process, then mathematical operations (such as adding and subtracting output values) are performed. With the statistic components (calculating mean, variance and skewness) existing
This might be a bit confusing at first, but more information is available in {ref}`sec:prob_stats`, and for the general process known as the Monte Carlo Method it is described in {ref}`sec:monte_carlo_method`.

Often times, in XLrisk, a user will pick a number of iterations, and through the Weak Law of Large Numbers[^2], the more trials, the more likely the sample average from those trials will converge into the true average value.

### Skew and Kurtosis

Skewness refers the asymmetry of the probability distribution of a
real-valued random variable about its mean. Skewness has two types.
*Positive* Skewness means that the distribution has a longer or fatter tail on the right side and the distribution's PMF is concentrated on the left. The mean is typically greater than the median.
*Negative* Skewness mean that The distribution has a longer or fatter tail on the left side and the mass of the distribution is concentrated on the right. The mean is typically less than the median.

Kurtosis refers to the "tailedness" (or "tail-heaviness")
of the probability distribution of a real-valued random variable.
It describes the sharpness of the peak and the weight of the tails
relative to a normal distribution. Kurtosis generally has three
forms. *Mesokurtic* ($\gamma \approx 0$) The distribution has a kurtosis
similar to that of a normal distribution. Tails are neither particularly
heavy nor light. *Leptokurtic* ($\gamma_2 > 0$) The distribution has heavier tails and a sharper peak than a normal distribution. This means there is a higher probability of extreme values (outliers). *Platykurtic*
($\gamma < 0$) The distribution has lighter tails and a flatter peak
than a normal distribution. This means there's a lower probability of extreme values,
and values tend to be more clustered around the mean but less
peaked than a normal distribution.

Skewness and Kurtosis are important for understanding the behavior of the results of the model, particularly when calculating the likelihood of extreme events, which is defined as risk in many different settings.

### Walk-Through (Virtual Textbook Only)

Here is a video describing probabilistic risk analysis using XLrisk that is similar to
one of the projects that is within this text that uses Python instead of XLrisk to ensure
that the reader will understand how Monte Carlo works.

```{iframe} https://www.youtube.com/watch?v=0RiEquwDjNg
This video is a about 20 minutes long and describes a project for 
building a risk estimate leveraging XLrisk. This is a part of the XLrisk
documentation.
```

:::

:::{tab-item} pyMC + Numpy + Scipy

### pyMC Functions

See {ref}`sec:software` for specifics on what might be useful for
pyMC, as it uses Numpy and Scipy functions to perform actions, and
some PyMC functions are included toward the end of the document.

### pyMC Forced Correlations

In pyMC, copulas are implemented through a five step process based on
{cite}`PyMCDocumentation`:

1. Define the marginal distributions (they can be any distribution that is univariate);
2. Transform the marginals into the inverse of the Gaussian CDF (this is known as the Probability integral transform).
3. Define the multivariate Gaussian instantiated with the desired correlation matrix between the marginals;
4. Then simulate (or obtain the actual values from) a multidimensional Gaussian.
5. Apply the probability integral transformation to the multidimensional Gaussian.
6. Replace the marginals of the multidimensional Gaussian into the desired marginal distributions from step 2.

An example of this transformation between two exponential random variables with parameters three and five is included below:

::::{admonition} Code
:class: dropdown

```{code} python
import matplotlib.pyplot as plt
import numpy as np
import pymc as pm
import seaborn as sns

from scipy.stats import expon, multivariate_normal, norm

# Step (1,2, and 3), define the multivariate Gaussian:
b_scale = 2
θ = {"a_dist": norm(), "b_dist": expon(scale=1 / b_scale), "rho": 0.9}

n_samples = 5000

# Step 4: sample the multivariate Gaussian:
mu = [0, 0]
cov = [[1, θ["rho"]], [θ["rho"], 1]]
x = multivariate_normal(mu, cov).rvs(n_samples, random_state=rng)
a_norm = x[:, 0]
b_norm = x[:, 1]

# Step 5: Apply the Transformation to the multivariate Gaussian:
a_unif = norm(loc=0, scale=1).cdf(a_norm)
b_unif = norm(loc=0, scale=1).cdf(b_norm)
sns.jointplot(x=a_unif, y=b_unif, height=6, kind="hex");

# Step 6.
a1 = ["a_dist"behavior].cdf(a)
b1 = ["b_dist"].cdf(b)
sns.jointplot(x=a1, y=b1, kind="hex", height=6);
```

::::

### pyMC Presentation of Results

The differences between PyMC and XLrisk are not based on fundamental mathematical differences, but based on interface changes between a spreadsheet and Python, as the laster tends to have a more terminal-focused UI/UX.

#### PyMC Statistics and Outputs

Generally, PyMC is generally considered a package for Bayesian statistics and analysis

### Skew and Kurtosis through Pandas

### Example of Using pyMC (Code with Explanation)


:::

::::

[^1]: It is important to note that the Wichmann-Hill random number generator is based on three Linear Congruential Generators (LCGs) that have different prime modulus (2^31-1, 8121, and 48271) for example that are then combined into one stream, this process was discussed in {cite}`Law:13`.

[^2]: In several simulation textbooks such as {cite}`Law:13` and {cite}`Banks:14`, they describe that this property is universal and inevitable through the _strong law of large numbers_, when that is not always true, particularly with some of the projects and labs in this text. An example of a limitation of the law of large number is that some CDFs do not have an expected value, but the weak law still holds in these cases.
